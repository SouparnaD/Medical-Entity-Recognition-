{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "cadec_twitter_bilstmCNNcrf_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYd8W44RDZIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "dataset_path = '/content/drive/My Drive/Colab Notebooks/MER/model_trial/dataset/CADEC.csv'\n",
        "dataset_path_twitter = '/content/drive/My Drive/Colab Notebooks/MER/model_trial/dataset/twitter_521.csv'\n",
        "BATCH_SIZE = 32  # Number of examples used in each iteration\n",
        "EPOCHS = 100 #Number of passes through entire dataset\n",
        "MAX_LEN = 60  # Max length of review (in words)\n",
        "EMBEDDING = 200# Dimension of word embedding vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzXTEgRQDZI2",
        "colab_type": "code",
        "outputId": "0cb66e0c-1bb6-4de5-9940-88b4c16dc84f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "data_twitter = pd.read_csv(dataset_path_twitter, names=['Sentence No', 'Word', 'Tag'], header=0)\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "data = data.drop(['Document ID'], axis=1)\n",
        "\n",
        "# Adding some of the twitter data in the cadec dataset to train better for the twitter data\n",
        "# data = data.append(data_twitter[0:5991], ignore_index=True)\n",
        "# # Remaining twitter data is for testing\n",
        "# data_twitter = data_twitter[5991:]\n",
        "# print(data_twitter.shape)\n",
        "\n",
        "data = data.fillna(method=\"ffill\")\n",
        "# data_twitter = data_twitter.fillna(method=\"ffill\")\n",
        "\n",
        "print(\"Number of sentences: \", len(data.groupby(['Sentence No'])))\n",
        "# print(\"Number of sentences in twitter : \", len(data_twitter.groupby(['Sentence No'])))\n",
        "\n",
        "\n",
        "words = list(set(data[\"Word\"].values))\n",
        "n_words = len(words)\n",
        "print(\"Number of words in the dataset: \", n_words)\n",
        "# words_twitter = list(set(data_twitter[\"Word\"].values))\n",
        "# n_words_twitter = len(words_twitter)\n",
        "# print(\"Number of words in the dataset: \", n_words_twitter)\n",
        "\n",
        "tags = list(set(data[\"Tag\"].values))\n",
        "print(\"Tags:\", tags)\n",
        "n_tags = len(tags)\n",
        "print(\"Number of Labels: \", n_tags)\n",
        "\n",
        "print(\"What the dataset looks like:\")\n",
        "# Show the first 10 rows\n",
        "data['Tag'].value_counts()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences:  7520\n",
            "Number of words in the dataset:  8327\n",
            "Tags: ['O', 'I-Symptom', 'B-Drug', 'B-ADR', 'B-Finding', 'B-Disease', 'B-Symptom', 'I-Finding', 'I-ADR', 'I-Disease', 'I-Drug']\n",
            "Number of Labels:  11\n",
            "What the dataset looks like:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O            101896\n",
              "I-ADR         10900\n",
              "B-ADR          5756\n",
              "B-Drug         1799\n",
              "I-Finding       477\n",
              "B-Finding       414\n",
              "I-Symptom       332\n",
              "B-Disease       282\n",
              "B-Symptom       268\n",
              "I-Drug          243\n",
              "I-Disease       213\n",
              "Name: Tag, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmYBWv2zDZJF",
        "colab_type": "code",
        "outputId": "a8e92e69-ce2c-41e2-eab5-ab58b8be265b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "from random import shuffle\n",
        "import random\n",
        "class SentenceGetter(object):\n",
        "    \"\"\"Class to Get the sentence in this format:\n",
        "    [(Token_1, Tag_1), ..., (Token_n, Tag_1)]\"\"\"\n",
        "    def __init__(self, data):\n",
        "        \"\"\"Args:\n",
        "            data is the pandas.DataFrame which contains the above dataset\"\"\"\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence No\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        \"\"\"Return one sentence\"\"\"\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "        \n",
        "getter = SentenceGetter(data)\n",
        "# getter_twitter = SentenceGetter(data_twitter)\n",
        "sent = getter.get_next()# Get all the sentences\n",
        "sentences = getter.sentences\n",
        "# sentences_twitter = getter_twitter.sentences\n",
        "\n",
        "\n",
        "# print(sentences_twitter)\n",
        "# Shuffling the cadec and twitter dataset for better training\n",
        "sentences = random.sample(sentences, len(sentences))\n",
        "print(len(sentences))\n",
        "sentences\n",
        "\n",
        "# Plot sentence by lenght\n",
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.title('Token per sentence')\n",
        "plt.xlabel('Len (number of token)')\n",
        "plt.ylabel('# samples')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7520\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZkVcrEwM7NC/x+I0K9/WK6a2AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "o_lT35CyDZJS",
        "colab_type": "code",
        "outputId": "62aad85e-1d91-40f5-cf31-761537284fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "X = [[w[0]for w in s] for s in sentences]\n",
        "new_X = []\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(MAX_LEN):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PAD\")\n",
        "    new_X.append(new_seq)\n",
        "\n",
        "new_X[15]\n",
        "\n",
        "# Mapping the string data to the numbers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tags2index = {t:i for i,t in enumerate(tags)}\n",
        "y = [[tags2index[w[1]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tags2index[\"O\"])\n",
        "y[15]\n",
        "\n",
        "#------------------------------------------------------------------- with twitter data\n",
        "# # Vocabulary Key:word -> Value:token_index\n",
        "# # The first 2 entries are reserved for PAD and UNK\n",
        "# word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
        "# word2idx[\"UNK\"] = 1 # Unknown words\n",
        "# word2idx[\"PAD\"] = 0 # Padding\n",
        "# word2idx\n",
        "# word2idx_twitter = {w: i + 2 for i, w in enumerate(words_twitter)}\n",
        "# word2idx_twitter[\"UNK\"] = 1 # Unknown words\n",
        "# word2idx_twitter[\"PAD\"] = 0 # Padding\n",
        "# word2idx_twitter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Vocabulary Key:token_index -> Value:word\n",
        "# idx2word_twitter = {i: w for w, i in word2idx_twitter.items()}\n",
        "# idx2word_twitter\n",
        "# idx2word = {i: w for w, i in word2idx.items()}\n",
        "# idx2word\n",
        "\n",
        "# # Vocabulary Key:Label/Tag -> Value:tag_index\n",
        "# # The first entry is reserved for PAD\n",
        "# tag2idx = {t: i+1 for i, t in enumerate(tags)}\n",
        "# tag2idx[\"PAD\"] = 0\n",
        "# tag2idx\n",
        "\n",
        "\n",
        "# # Vocabulary Key:tag_index -> Value:Label/Tag\n",
        "# idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "# idx2tag\n",
        "\n",
        "\n",
        "# print(\"The word Drugs is identified by the index: {}\".format(word2idx[\"Drug\"]))\n",
        "# print(\"The labels B-Symptom(which defines Symptom related Enitities) is identified by the index: {}\".format(tag2idx[\"B-Symptom\"]))\n",
        "\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# # Convert each sentence from list of Token to list of word_index\n",
        "\n",
        "# X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "# # Padding each sentence to have the same lenght\n",
        "# X = pad_sequences(maxlen=MAX_LEN, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
        "\n",
        "# X_twitter = [[word2idx_twitter[w[0]] for w in s] for s in sentences_twitter]\n",
        "# # Padding each sentence to have the same lenght\n",
        "# X_twitter = pad_sequences(maxlen=MAX_LEN, sequences=X_twitter, padding=\"post\", value=word2idx_twitter[\"PAD\"])\n",
        "\n",
        "\n",
        "# # Convert Tag/Label to tag_index\n",
        "# y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "# # Padding each sentence to have the same length\n",
        "# y = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
        "# from keras.utils import to_categorical\n",
        "# # One-Hot encode\n",
        "# y = [to_categorical(i, num_classes=n_tags+1) for i in y]  # n_tags+1(PAD)\n",
        "\n",
        "\n",
        "# y_twitter = [[tag2idx[w[1]] for w in s] for s in sentences_twitter]\n",
        "# # Padding each sentence to have the same length\n",
        "# y_twitter = pad_sequences(maxlen=MAX_LEN, sequences=y_twitter, padding=\"post\", value=tag2idx[\"PAD\"])\n",
        "# from keras.utils import to_categorical\n",
        "# # One-Hot encode\n",
        "# y_twitter = [to_categorical(i, num_classes=n_tags+1) for i in y_twitter]  # n_tags+1(PAD)\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# # X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)  #this was used when CADEC was used for trraining, testing both\n",
        "\n",
        "# X_tr, X_te = X, X_twitter\n",
        "# y_tr, y_te = y, y_twitter\n",
        "\n",
        "\n",
        "# X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape\n",
        "\n",
        "\n",
        "# # print(\" sentence is : \")\n",
        "# # (print(idx2word_twitter[word]) for word in sentences_twitter)\n",
        "\n",
        "# print('Raw Sample: ', ' '.join([w[0] for w in sentences[0]]))\n",
        "# print('Raw Label: ', ' '.join([w[1] for w in sentences[0]]))\n",
        "# print('Raw Sample: ', ' '.join([w[0] for w in sentences_twitter[0]]))\n",
        "# print('Raw Label: ', ' '.join([w[1] for w in sentences_twitter[0]]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7idoPxjQDZJd",
        "colab_type": "code",
        "outputId": "8e81cb0e-561c-4e76-f839-feb4e0a64050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "new_X = new_X[:1000]\n",
        "y = y[:1000]\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(new_X, y, test_size=0.1, random_state=2018)\n",
        "print(np.array(X_tr).shape)\n",
        "print(np.array(X_te).shape)\n",
        "\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.tables_initializer())\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "def ElmoEmbedding(x):\n",
        "    return elmo_model(inputs={\"tokens\": tf.squeeze(tf.cast(x,    tf.string)),\"sequence_len\": tf.constant(BATCH_SIZE*[MAX_LEN])\n",
        "                     },\n",
        "                      signature=\"tokens\",\n",
        "                      as_dict=True)[\"elmo\"]\n",
        "\n",
        "from keras.models import Model, Input\n",
        "from keras.layers.merge import add\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
        "input_text = Input(shape=(MAX_LEN,), dtype=tf.string)\n",
        "embedding = Lambda(ElmoEmbedding, output_shape=(MAX_LEN, 1024))(input_text)\n",
        "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
        "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
        "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
        "model = Model(input_text, out)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------- with twitter data\n",
        "# from keras.models import Model, Input\n",
        "# from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "# from keras_contrib.layers import CRF\n",
        "\n",
        "# # Model definition\n",
        "# input = Input(shape=(MAX_LEN,))\n",
        "# input\n",
        "# model = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n",
        "#                   input_length=MAX_LEN, mask_zero=True)(input)  # default: 20-dim embedding\n",
        "# model = Bidirectional(LSTM(units=50, return_sequences=True,\n",
        "#                            recurrent_dropout=0.1))(model)  # variational biLSTM\n",
        "# model = TimeDistributed(Dense(50, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\n",
        "# crf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\n",
        "# out = crf(model)  # output\n",
        "\n",
        "# model = Model(input, out)\n",
        "# model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
        "\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900, 60)\n",
            "(100, 60)\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 60)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 60, 1024)     0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 60, 1024)     6295552     lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 60, 1024)     6295552     bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 60, 1024)     0           bidirectional_5[0][0]            \n",
            "                                                                 bidirectional_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 60, 11)       11275       add_3[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 12,602,379\n",
            "Trainable params: 12,602,379\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LOwH4Ea7LLr",
        "colab_type": "code",
        "outputId": "f7eb0c70-927a-496f-9d05-5474d697befe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "X_tr, X_val = X_tr[:28*BATCH_SIZE], X_tr[-3*BATCH_SIZE:]\n",
        "y_tr, y_val = y_tr[:28*BATCH_SIZE], y_tr[-3*BATCH_SIZE:]\n",
        "y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
        "y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\n",
        "\n",
        "history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),batch_size=BATCH_SIZE, epochs=3, verbose=1)\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------- withh twitter data\n",
        "# # history = model.fit(X_tr, np.array(y_tr), batch_size=BATCH_SIZE, epochs=EPOCHS,validation_split=0.1, verbose=2) #this was used when CADEC was used for training, testing both\n",
        "# history = model.fit(X_tr, np.array(y_tr), batch_size=BATCH_SIZE, epochs=EPOCHS,validation_split=0.0, verbose=2)\n",
        "\n",
        "# # Eval\n",
        "# pred_cat = model.predict(X_te)\n",
        "# print(pred_cat.shape)\n",
        "# pred = np.argmax(pred_cat, axis=-1)\n",
        "# y_te_true = np.argmax(y_te, -1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 896 samples, validate on 96 samples\n",
            "Epoch 1/3\n",
            "896/896 [==============================] - 563s 629ms/step - loss: 0.3034 - acc: 0.9201 - val_loss: 0.1071 - val_acc: 0.9622\n",
            "Epoch 2/3\n",
            "896/896 [==============================] - 557s 622ms/step - loss: 0.1009 - acc: 0.9693 - val_loss: 0.0874 - val_acc: 0.9675\n",
            "Epoch 3/3\n",
            "896/896 [==============================] - 556s 621ms/step - loss: 0.0838 - acc: 0.9739 - val_loss: 0.0716 - val_acc: 0.9745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nFM0WunDmp1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a87c2d0-0cd5-47c4-9f74-5bfd8692751d"
      },
      "source": [
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "X_te = X_te[:3*BATCH_SIZE]\n",
        "test_pred = model.predict(np.array(X_te), verbose=1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96/96 [==============================] - 36s 376ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjFbAjwn93Dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "ec469a52-8581-4271-f29d-f05b175d7b49"
      },
      "source": [
        "idx2tag = {i: w for w, i in tags2index.items()}\n",
        "\n",
        "\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i].replace(\"PADword\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "    \n",
        "def test2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            out_i.append(idx2tag[p].replace(\"PADword\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "    \n",
        "pred_labels = pred2label(test_pred)\n",
        "\n",
        "test_labels = test2label(y_te[:3*32])\n",
        "print(classification_report(test_labels, pred_labels))\n",
        "print(np.array(pred_labels).shape)\n",
        "print(np.array(test_labels).shape)\n",
        "\n",
        "count = 0\n",
        "total = 0\n",
        "for i in range(len(test_labels)):\n",
        "  if test_labels[i] == pred_labels[i]:\n",
        "    count += 1\n",
        "  total += 1\n",
        "print(\" accuracy : \",count/total)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           precision    recall  f1-score   support\n",
            "\n",
            "      ADR       0.40      0.55      0.46        66\n",
            "  Disease       0.00      0.00      0.00         1\n",
            "     Drug       0.87      0.68      0.76        19\n",
            "  Symptom       0.00      0.00      0.00         6\n",
            "  Finding       0.00      0.00      0.00         1\n",
            "\n",
            "micro avg       0.47      0.53      0.50        93\n",
            "macro avg       0.46      0.53      0.49        93\n",
            "\n",
            "(96, 60)\n",
            "(96, 60)\n",
            " accuracy :  0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9zEUZi7hSPO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "cd2728d3-efa0-4cff-c3b9-2284e4dc4d73"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUzrFgO6hTQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSWvgJqf5CSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "992VTd9l5a8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install keras==2.2.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r0nZxrx6XoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install sklearn-crfsuite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSU7bI86ERV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "68e35a87-9535-434e-e3b6-d2ca1b8ac27b"
      },
      "source": [
        "# !pip install seqeval"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.3)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=042186aa1e059b07de5997b09b0fc3358c1046926e86316fc0323c6034b666a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxJlRd0pESF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8dd66064-0d4a-400a-cb92-af1039190972"
      },
      "source": [
        "!pip install sklearn_crfsuite"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn_crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.12.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.28.1)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.5)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.6 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYrdv__6EgE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}